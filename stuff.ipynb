{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCEAN to MBTI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix\n",
    "[Статья](https://reunir.unir.net/bitstream/handle/123456789/9799/ijimai20142_7_4_pdf_92105.pdf?sequence=1&isAllowed=y)\n",
    "\n",
    "| | Extraversion | Openness | Agreeableness | Conscientiousness | Neuroticism |\n",
    "| --- | ------------ | -------- | ------------- | ----------------- | ----------- |\n",
    "| E-I | -0.74 | 0.03 | -0.03 | 0.08 | 0.16 |\n",
    "| S-N | 0.10 | 0.72 | 0.04 | -0.15 | -0.06 |\n",
    "| T-F | 0.19 | 0.02 | 0.44 | -0.15 | 0.06 |\n",
    "| J-P | 0.15 | 0.30 | -0.06 | -0.49 | 0.11 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = pd.DataFrame({\n",
    "    \"E-I\": np.array([-0.74, 0.03, -0.03, 0.08, 0.16]),\n",
    "    \"S-N\": np.array([0.10, 0.72, 0.04, -0.15, -0.06]),\n",
    "    \"T-F\": np.array([0.19, 0.02, 0.44, -0.15, 0.06]),\n",
    "    \"J-P\": np.array([0.15, 0.30, -0.06, -0.49, 0.11]),\n",
    "}, index = ['Extraversion', 'Openness', 'Agreeableness', 'Conscientiousness', 'Neuroticism'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocean_to_mbti(ocean_vector):\n",
    "    \"\"\"\n",
    "    Конвертирует OCEAN-представление в MBTI на основе корреляционной матрицы.\n",
    "    :param ocean_vector: np.array с пятью значениями OCEAN в порядке [E, O, A, C, N]. Значение от 0 до 1 (степень выраженности черты)\n",
    "    :return: строка с типом MBTI\n",
    "    \"\"\"\n",
    "    mbti_vector = {}\n",
    "    \n",
    "    # Рассчитываем значение каждой шкалы MBTI\n",
    "    for scale, weights in correlation_matrix.items():\n",
    "        mbti_vector[scale] = np.dot(ocean_vector, weights)\n",
    "    \n",
    "    mbti_type = (\n",
    "        \"E\" if mbti_vector[\"E-I\"] > 0.5 else \"I\",\n",
    "        \"N\" if mbti_vector[\"S-N\"] > 0.5 else \"S\",\n",
    "        \"F\" if mbti_vector[\"T-F\"] > 0.5 else \"T\",\n",
    "        \"P\" if mbti_vector[\"J-P\"] > 0.5 else \"J\",\n",
    "    )\n",
    "    \n",
    "    return \"\".join(mbti_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_vector = np.array([0.6, 0.7, 0.2, 0.1, 0.3])\n",
    "mbti_type = ocean_to_mbti(ocean_vector)\n",
    "print(\"Тип личности MBTI:\", mbti_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_emb = []\n",
    "for i in os.listdir('data/train/video_embeddings/'):\n",
    "    emb = np.load(f'data/train/video_embeddings/{i}')\n",
    "    candidates_emb.append(emb)\n",
    "candidates_emb = np.array(candidates_emb)\n",
    "\n",
    "vacancies_emb = []\n",
    "for i in os.listdir('data/embeddings/vacancies/embeddings'):\n",
    "    emb = np.load(f'data/embeddings/vacancies/embeddings/{i}')\n",
    "    vacancies_emb.append(emb)\n",
    "vacancies_emb = np.array(vacancies_emb)\n",
    "\n",
    "types_emb = np.load('data/embeddings/OCEAN_embeddings.npy')\n",
    "\n",
    "vacancies_data = pd.read_csv('data/tables/vacancies.csv')\n",
    "types_data = pd.read_csv('data/tables/personality.csv')\n",
    "with open('data/train/transcription.pkl', 'rb') as file:\n",
    "    candidates_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 4, 16, 1, 28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(vector_a, vector_b):\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    magnitude_a = np.linalg.norm(vector_a)\n",
    "    magnitude_b = np.linalg.norm(vector_b)\n",
    "    cosine_of_angle = dot_product / (magnitude_a * magnitude_b)\n",
    "    return cosine_of_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06935373"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_angle(candidates_emb[0], types_emb[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_emb_ = candidates_emb / np.linalg.norm(candidates_emb, axis=1,  keepdims=True)\n",
    "candidates_emb_ = candidates_emb / np.linalg.norm(candidates_emb, axis=1,  keepdims=True)\n",
    "types_emb_ = types_emb / np.linalg.norm(types_emb, axis=1,  keepdims=True)\n",
    "\n",
    "cosine_similarity_matrix = np.dot(candidates_emb_, types_emb_.T)  # Результат [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 5)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(12, 4), sharey=True, sharex=True)\n",
    "\n",
    "for i in range(5):\n",
    "    axs[i].hist(cosine_similarity_matrix[:, i], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6000, 5), (2277, 5))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_candidates_matrix = candidates_emb.dot(types_emb.T)\n",
    "similarity_vacancies_matrix = vacancies_emb.dot(types_emb.T)\n",
    "similarity_candidates_matrix.shape, similarity_vacancies_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (candidate, speech) in enumerate(candidates_data.items()):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(candidate, speech)\n",
    "\n",
    "    type_vector = similarity_matrix[i]\n",
    "    type_vector = map(lambda x: round(x.item(), 2), type_vector)\n",
    "    print(dict(zip(['O', 'C', 'E', 'A', 'N'], type_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancies_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from src.modelling.get_video_embedding import extract_frames, extract_frames_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch16-16-frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/5044 [00:02<20:11,  4.16it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define parameters\n",
    "batch_size = 8  # Set your desired batch size\n",
    "num_frames = 16\n",
    "C, W, H = 3, 224, 224  # Example dimensions for RGB images resized to 224x224\n",
    "\n",
    "# Define a transform to resize and normalize images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((W, H)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def load_videos(data_dir):\n",
    "    video_tensors = []\n",
    "    \n",
    "    c = 0\n",
    "    # Iterate through each video_id folder\n",
    "    for video_id in tqdm(os.listdir(data_dir)):\n",
    "        video_path = os.path.join(data_dir, video_id)\n",
    "        \n",
    "        if os.path.isdir(video_path):\n",
    "            frames = []\n",
    "            \n",
    "            # Load each image in the video_id folder\n",
    "            for img_name in sorted(os.listdir(video_path)):\n",
    "                img_path = os.path.join(video_path, img_name)\n",
    "                \n",
    "                if img_name.endswith('.jpg'):\n",
    "                    img = Image.open(img_path).convert('RGB')  # Ensure image is in RGB format\n",
    "                    img_tensor = transform(img)\n",
    "                    frames.append(img_tensor)\n",
    "            \n",
    "            frames.append(img_tensor)\n",
    "            if len(frames) == num_frames:\n",
    "                # Stack frames to create a tensor of shape (num_frames, C, W, H)\n",
    "                video_tensor = torch.stack(frames)\n",
    "                video_tensors.append(video_tensor)\n",
    "        \n",
    "        c += 1\n",
    "\n",
    "        if c == 10:\n",
    "            break\n",
    "\n",
    "    return video_tensors\n",
    "\n",
    "# Load all videos and create batches\n",
    "all_video_tensors = load_videos('data/train/frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Create batches of tensors\n",
    "batches = []\n",
    "for i in range(0, len(all_video_tensors), batch_size):\n",
    "    batch = all_video_tensors[i:i + batch_size]\n",
    "    if len(batch) == batch_size:  # Ensure full batch size\n",
    "        batches.append(torch.stack(batch))  # Stack to create (batch_size, num_frames, C, W, H)\n",
    "\n",
    "# Example: Accessing the first batch\n",
    "if batches:\n",
    "    first_batch = batches[0]\n",
    "    print(first_batch.shape)  # Should print: torch.Size([batch_size, num_frames, C, W, H])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model.get_video_features(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|██████████| 6000/6000 [00:00<00:00, 45013.32it/s]\n"
     ]
    }
   ],
   "source": [
    "input_dir = 'data/train/video'\n",
    "output_dir = 'data/train/frames'\n",
    "\n",
    "video_files = [f for f in os.listdir(input_dir) if f.endswith('.mp4')]\n",
    "    \n",
    "c = 0\n",
    "for video_file in tqdm(video_files, desc=\"Processing videos\"):\n",
    "    video_path = os.path.join(input_dir, video_file)\n",
    "    video_id = os.path.splitext(os.path.basename(video_path))[0]\n",
    "\n",
    "    if len(os.listdir(os.path.join(output_dir, video_id))) < 15:\n",
    "        print(video_id)\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8192\n",
    "num_frames, H, W = 16, 28, 28\n",
    "256 * (num_frames // 8) * (H // 6) * (W // 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9216"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "4608 * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import autocast\n",
    "from cosmos_tokenizer.video_lib import CausalVideoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login, snapshot_download\n",
    "# import os\n",
    "\n",
    "# login(token=\"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\", add_to_git_credential=True)\n",
    "# model_names = [\n",
    "#         \"Cosmos-Tokenizer-CI8x8\",\n",
    "#         \"Cosmos-Tokenizer-CI16x16\",\n",
    "#         \"Cosmos-Tokenizer-CV4x8x8\",\n",
    "#         \"Cosmos-Tokenizer-CV8x8x8\",\n",
    "#         \"Cosmos-Tokenizer-CV8x16x16\",\n",
    "#         \"Cosmos-Tokenizer-DI8x8\",\n",
    "#         \"Cosmos-Tokenizer-DI16x16\",\n",
    "#         \"Cosmos-Tokenizer-DV4x8x8\",\n",
    "#         \"Cosmos-Tokenizer-DV8x8x8\",\n",
    "#         \"Cosmos-Tokenizer-DV8x16x16\",\n",
    "# ]\n",
    "# for model_name in model_names:\n",
    "#     hf_repo = \"nvidia/\" + model_name\n",
    "#     local_dir = \"pretrained_ckpts/\" + model_name\n",
    "#     os.makedirs(local_dir, exist_ok=True)\n",
    "#     print(f\"downloading {model_name}...\")\n",
    "#     snapshot_download(repo_id=hf_repo, local_dir=local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем трансформации для изменения размера и нормализации изображений\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, num_frames=16):\n",
    "        self.data_dir = data_dir\n",
    "        self.video_ids = [video_id for video_id in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, video_id))]\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames  # Фиксированное количество кадров\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_id = self.video_ids[idx]\n",
    "        video_path = os.path.join(self.data_dir, video_id)\n",
    "        \n",
    "        frames = []\n",
    "        frame_files = sorted(os.listdir(video_path))\n",
    "        \n",
    "        # Выбираем первые `num_frames` или заполняем, если меньше\n",
    "        if len(frame_files) >= self.num_frames:\n",
    "            selected_files = frame_files[:self.num_frames]\n",
    "        else:\n",
    "            # Если кадров меньше, дополняем последним кадром\n",
    "            selected_files = frame_files + [frame_files[-1]] * (self.num_frames - len(frame_files))\n",
    "\n",
    "        for img_name in selected_files:\n",
    "            img_path = os.path.join(video_path, img_name)\n",
    "            if img_name.endswith('.jpg'):\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                frames.append(img)\n",
    "        \n",
    "        # Создаем тензор видео (num_frames, C, H, W)\n",
    "        video_tensor = torch.stack(frames)\n",
    "        # Преобразуем к форме (C, num_frames, H, W)\n",
    "        video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "        return video_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры\n",
    "data_dir = 'data/train/frames'\n",
    "batch_size = 4  # Выберите подходящий размер батча\n",
    "\n",
    "# Создаем датасет и DataLoader\n",
    "dataset = VideoDataset(data_dir, transform=transform, num_frames=16)  # Фиксируем количество кадров на 16\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузите модель на GPU (если доступен)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = CausalVideoTokenizer(checkpoint_enc=f'pretrained_ckpts/Cosmos-Tokenizer-CV4x8x8/encoder.jit').to(device)\n",
    "\n",
    "# Переносим batch на устройство, где находится модель\n",
    "batch = batch.to(device)\n",
    "batch = batch.to(torch.float32)\n",
    "\n",
    "with autocast():\n",
    "    with torch.no_grad():\n",
    "        # [batch_size, num_frames, hid_dim, 28, 28]\n",
    "        out = encoder.encode(batch)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 5, 28, 28])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, dataloader, num_epochs=20, learning_rate=1e-3, device='cuda'):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Прямой проход\n",
    "            reconstructed, latent = model(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            \n",
    "            # Обратное распространение\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
