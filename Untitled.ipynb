{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d50075db-fa5d-4293-ac8a-7fd1c84e0881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 384)\n",
      "(5, 384)\n",
      "(2277, 384)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def build_matrix(\n",
    "    candidates_path='data/embeddings/candidates_train_text_embeddings.npy',\n",
    "    ocean_path='data/embeddings/OCEAN_embeddings.npy',\n",
    "    vacancies_path='data/embeddings/vacancies_embeddings.npy'\n",
    "):\n",
    "  \n",
    "    candidates = np.load(candidates_path)\n",
    "    print(candidates.shape)\n",
    "    ocean = np.load(ocean_path)\n",
    "    print(ocean.shape)\n",
    "    vacancies = np.load(vacancies_path)\n",
    "    print(vacancies.shape)\n",
    "\n",
    "    C = cosine_distances(candidates, ocean)\n",
    "    P = cosine_distances(vacancies, ocean)\n",
    "\n",
    "    final_matrix = C.dot(P.T) + (1 - C).dot(1 - P.T)\n",
    "    \n",
    "    return final_matrix, C, P\n",
    "\n",
    "def filter_matrix(final_matrix, threshold=0.7):\n",
    "\n",
    "    normalized_matrix = final_matrix.copy()\n",
    "    \n",
    "    row_min = normalized_matrix.min(axis=1).reshape(-1, 1)\n",
    "    row_max = normalized_matrix.max(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    denominator = row_max - row_min\n",
    "    denominator[denominator == 0] = 1\n",
    "    \n",
    "    normalized_matrix = (normalized_matrix - row_min) / denominator\n",
    "    \n",
    "    normalized_matrix[normalized_matrix < threshold] = 0\n",
    "    \n",
    "    return normalized_matrix\n",
    "\n",
    "a, C, P = build_matrix()\n",
    "b = filter_matrix(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "551dd333-9748-44dc-a505-588c423853bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется устройство: Tesla V100-SXM3-32GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Используется устройство: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"GPU не доступен. Используется CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e49ad2b9-d974-4aee-beeb-869453d40ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embeddings_path):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "    return embeddings\n",
    "\n",
    "def load_candidates_embeddings(data_folder, max_users=1500):\n",
    "    candidates_train_path = os.path.join(data_folder, 'embeddings', 'candidates_train_text_embeddings.npy')\n",
    "    candidates_val_path = os.path.join(data_folder, 'embeddings', 'candidates_val_text_embeddings.npy')\n",
    "    \n",
    "    candidates_train = load_embeddings(candidates_train_path)\n",
    "    candidates_val = load_embeddings(candidates_val_path)\n",
    "    \n",
    "    # Объединяем обучающие и валидационные эмбеддинги\n",
    "    candidates = torch.cat([candidates_train, candidates_val], dim=0)\n",
    "    \n",
    "    # Ограничиваем количество пользователей до max_users\n",
    "    if candidates.shape[0] > max_users:\n",
    "        candidates = candidates[:max_users]\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def load_jobs_embeddings(data_folder):\n",
    "    jobs_path = os.path.join(data_folder, 'embeddings', 'vacancies_embeddings.npy')\n",
    "    jobs_embeddings = load_embeddings(jobs_path)\n",
    "    return jobs_embeddings\n",
    "\n",
    "def load_ocean_embeddings(data_folder):\n",
    "    ocean_path = os.path.join(data_folder, 'embeddings', 'OCEAN_embeddings.npy')\n",
    "    ocean_embeddings = load_embeddings(ocean_path)\n",
    "    return ocean_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7fd3ba51-fd79-4103-9567-0632af100d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_video_emb = []\n",
    "for i in os.listdir('data/validation/video_embeddings/'):\n",
    "    emb = np.load(f'data/validation/video_embeddings/{i}')\n",
    "    candidates_video_emb.append(emb[0])\n",
    "candidates_video_emb = np.array(candidates_video_emb)\n",
    "\n",
    "candidates_text_emb = []\n",
    "for i in os.listdir('data/validation/text_embeddings/'):\n",
    "    emb = np.load(f'data/validation/text_embeddings/{i}')\n",
    "    candidates_text_emb.append(emb)\n",
    "candidates_text_emb = np.array(candidates_text_emb)\n",
    "types_emb = np.load('data/embeddings/OCEAN_embeddings.npy')\n",
    "candidates_video_emb_norm = candidates_video_emb / np.linalg.norm(candidates_video_emb, axis=1, keepdims=True)\n",
    "candidates_text_emb_norm = candidates_text_emb / np.linalg.norm(candidates_text_emb, axis=1, keepdims=True)\n",
    "types_emb_norm = types_emb / np.linalg.norm(types_emb, axis=1, keepdims=True)\n",
    "\n",
    "candidates_emb = (candidates_video_emb_norm + candidates_text_emb_norm) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4e34308b-5914-4d53-99a3-58bc95344d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка эмбеддингов пользователей...\n",
      "Эмбеддинги пользователей загружены: torch.Size([2000, 384])\n",
      "Загрузка эмбеддингов OCEAN типов личности...\n",
      "Эмбеддинги OCEAN загружены: torch.Size([5, 384])\n",
      "Загрузка эмбеддингов вакансий...\n",
      "Эмбеддинги вакансий загружены: torch.Size([2277, 384])\n"
     ]
    }
   ],
   "source": [
    "# Путь к папке с данными\n",
    "data_folder = 'data'\n",
    "\n",
    "# Загрузка эмбеддингов\n",
    "print(\"Загрузка эмбеддингов пользователей...\")\n",
    "candidates = torch.tensor(candidates_emb, dtype=torch.float32)\n",
    "#candidates = load_candidates_embeddings(data_folder)\n",
    "print(f\"Эмбеддинги пользователей загружены: {candidates.shape}\")\n",
    "\n",
    "print(\"Загрузка эмбеддингов OCEAN типов личности...\")\n",
    "ocean = load_ocean_embeddings(data_folder)\n",
    "print(f\"Эмбеддинги OCEAN загружены: {ocean.shape}\")\n",
    "\n",
    "print(\"Загрузка эмбеддингов вакансий...\")\n",
    "jobs = load_jobs_embeddings(data_folder)\n",
    "print(f\"Эмбеддинги вакансий загружены: {jobs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3bd951ec-d639-4a70-838b-0f4bdb4c9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def build_graph(candidates, ocean, jobs):\n",
    "    num_candidates = candidates.shape[0]\n",
    "    num_ocean = ocean.shape[0]\n",
    "    num_jobs = jobs.shape[0]\n",
    "    \n",
    "    # Объединяем все эмбеддинги в один\n",
    "    x = torch.cat([candidates, ocean, jobs], dim=0)\n",
    "    \n",
    "    # Создаём рёбра\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    \n",
    "    # Пользователи ↔ OCEAN\n",
    "    for user_idx in range(num_candidates):\n",
    "        for ocean_idx in range(num_ocean):\n",
    "            src = user_idx\n",
    "            dst = num_candidates + ocean_idx\n",
    "            edge_index.append([src, dst])\n",
    "            # Вычисляем косинусное расстояние\n",
    "            distance = 1 - cosine_similarity(candidates[user_idx].unsqueeze(0), ocean[ocean_idx].unsqueeze(0))[0][0]\n",
    "            edge_attr.append(distance)\n",
    "            # Добавляем обратное ребро\n",
    "            edge_index.append([dst, src])\n",
    "            edge_attr.append(distance)\n",
    "    \n",
    "    # Вакансии ↔ OCEAN\n",
    "    for job_idx in range(num_jobs):\n",
    "        for ocean_idx in range(num_ocean):\n",
    "            src = num_candidates + ocean_idx\n",
    "            dst = num_candidates + num_ocean + job_idx\n",
    "            edge_index.append([src, dst])\n",
    "            distance = 1 - cosine_similarity(jobs[job_idx].unsqueeze(0), ocean[ocean_idx].unsqueeze(0))[0][0]\n",
    "            edge_attr.append(distance)\n",
    "            # Добавляем обратное ребро\n",
    "            edge_index.append([dst, src])\n",
    "            edge_attr.append(distance)\n",
    "    \n",
    "    # Пользователи ↔ Вакансии (без весов)\n",
    "    for user_idx in range(num_candidates):\n",
    "        for job_idx in range(num_jobs):\n",
    "            src = user_idx\n",
    "            dst = num_candidates + num_ocean + job_idx\n",
    "            edge_index.append([src, dst])\n",
    "            edge_attr.append(0.0)  # Вес 0 для отсутствующих весов\n",
    "            # Добавляем обратное ребро\n",
    "            edge_index.append([dst, src])\n",
    "            edge_attr.append(0.0)\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0bc53900-0851-4bf7-b5f7-5eca1b5cb3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Построение графа...\n",
      "Граф создан: Data(x=[4282, 384], edge_index=[2, 9150770], edge_attr=[9150770])\n"
     ]
    }
   ],
   "source": [
    "print(\"Построение графа...\")\n",
    "data = build_graph(candidates, ocean, jobs)\n",
    "print(f\"Граф создан: {data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d2f7bf71-947b-4a85-9fb2-1bbaf0bfdfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class GraphSAGEModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, dropout_prob=0.2):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels[0]))\n",
    "        self.convs.append(SAGEConv(hidden_channels[0], hidden_channels[1]))\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.convs.append(SAGEConv(hidden_channels[1], hidden_channels[2]))\n",
    "        self.convs.append(SAGEConv(hidden_channels[2], out_channels))\n",
    "\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0e8cdb15-f75a-4cfc-bb4e-9e26d3656b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "def get_link_labels(edge_index, num_nodes, num_neg_samples=4000000):\n",
    "\n",
    "    #Генерирует положительные и отрицательные примеры для задачи предсказания рёбер.\n",
    "    #Мы будем использовать задачу Link Prediction для предсказания существования рёбер между пользователями и вакансиями.\n",
    "    \n",
    "    pos_edge_index = edge_index\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=pos_edge_index,\n",
    "        num_nodes=num_nodes,\n",
    "        num_neg_samples=num_neg_samples\n",
    "    )\n",
    "    return pos_edge_index, neg_edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b7c4e412-5747-48c3-a9c0-a9a343dd7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class LinkPredictionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, num_neg_samples=4000000):\n",
    "        self.data = data\n",
    "        self.num_neg_samples = num_neg_samples\n",
    "        self.pos_edge_index, self.neg_edge_index = get_link_labels(\n",
    "            data.edge_index,\n",
    "            num_nodes=data.num_nodes,\n",
    "            num_neg_samples=num_neg_samples\n",
    "        )\n",
    "        # Перемещаем рёбра на GPU\n",
    "        self.pos_edge_index = self.pos_edge_index.to(device)\n",
    "        self.neg_edge_index = self.neg_edge_index.to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1  # В данном случае весь граф обучается за один раз\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pos_edge_index, self.neg_edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ede45fb0-75bc-4155-80d9-cde4fb6f3be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data, pos_edge_index, neg_edge_index):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "\n",
    "    pos_pred = (out[pos_edge_index[0]] * out[pos_edge_index[1]]).sum(dim=1)\n",
    "\n",
    "    neg_pred = (out[neg_edge_index[0]] * out[neg_edge_index[1]]).sum(dim=1)\n",
    "\n",
    "    \n",
    "    # Логистическая регрессия\n",
    "    pos_loss = F.binary_cross_entropy_with_logits(pos_pred, torch.ones(pos_pred.size(0), device=device))\n",
    "    neg_loss = F.binary_cross_entropy_with_logits(neg_pred, torch.zeros(neg_pred.size(0), device=device))\n",
    "\n",
    "    \n",
    "    loss = pos_loss + neg_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "\n",
    "def test(model, data, pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index, data.edge_attr)\n",
    "        \n",
    "        # Предсказания для положительных и отрицательных рёбер\n",
    "        pos_pred = (out[pos_edge_index[0]] * out[pos_edge_index[1]]).sum(dim=1)\n",
    "        neg_pred = (out[neg_edge_index[0]] * out[neg_edge_index[1]]).sum(dim=1)\n",
    "        \n",
    "        # Объединяем предсказания и метки\n",
    "        preds = torch.cat([pos_pred, neg_pred], dim=0)\n",
    "        labels = torch.cat([torch.ones(pos_pred.size(0), device=device), torch.zeros(neg_pred.size(0), device=device)], dim=0)\n",
    "        \n",
    "        # Применяем сигмоиду для перевода предсказаний в диапазон [0, 1]\n",
    "        preds = torch.sigmoid(preds)\n",
    "        preds = preds.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        \n",
    "        # Вычисление AUC\n",
    "        auc = roc_auc_score(labels, preds)\n",
    "        \n",
    "    return auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f70e3f96-978e-44b2-91b7-89e8ecf3dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def main_training_loop():\n",
    "    # Создание графа\n",
    "    #print(\"Построение графа...\")\n",
    "    #data = build_graph(candidates, ocean, jobs)\n",
    "    #print(f\"Граф создан: {data}\")\n",
    "    \n",
    "    num_nodes = data.num_nodes\n",
    "    \n",
    "    # Создание датасета и загрузчика данных\n",
    "    dataset = LinkPredictionDataset(data, num_neg_samples=4000000)  # Увеличиваем отрицательные примеры\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    # Определение модели\n",
    "    in_channels = data.x.shape[1]\n",
    "    hidden_channels = [384, 256, 128]  # Указанные размеры для скрытых слоев\n",
    "    out_channels = 64  # Размерность выхода\n",
    "    model = GraphSAGEModel(in_channels, hidden_channels, out_channels, dropout_prob=0.2).to(device)\n",
    "    \n",
    "    # Перевод данных на GPU\n",
    "    data.x = data.x.to(device)\n",
    "    data.edge_attr = data.edge_attr.to(device)\n",
    "    data.edge_index = data.edge_index.to(device)\n",
    "\n",
    "    # Оптимизатор и планировщик\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # Уменьшаем lr каждые 20 эпох\n",
    "\n",
    "    # Очистка памяти\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Обучение\n",
    "    epochs = 200\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for pos_edge, neg_edge in dataloader:\n",
    "            pos_edge = pos_edge[0].to(device)\n",
    "            neg_edge = neg_edge[0].to(device)\n",
    "            loss = train(model, optimizer, data, pos_edge, neg_edge)\n",
    "        \n",
    "        # Обновление планировщика\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Оценка модели каждые 10 эпох\n",
    "        if epoch % 10 == 0:\n",
    "            auc = test(model, data, pos_edge, neg_edge)\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}, AUC: {auc:.4f}')\n",
    "    \n",
    "    # Сохранение модели\n",
    "    model_save_path = 'graphsage_model1.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Модель сохранена по пути: {model_save_path}\")\n",
    "    \n",
    "    return model, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4f8edfe2-ca73-4dfc-bef3-393f9583dc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.x device: cuda:0\n",
      "data.edge_attr device: cuda:0\n",
      "data.edge_index device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"data.x device: {data.x.device}\")\n",
    "print(f\"data.edge_attr device: {data.edge_attr.device}\")\n",
    "print(f\"data.edge_index device: {data.edge_index.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ffb368-f07b-4026-b472-5059e2a1b1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 62.5251, AUC: 0.5000\n",
      "Epoch 20, Loss: 1.7355, AUC: 0.5646\n",
      "Epoch 30, Loss: 3.5711, AUC: 0.2213\n",
      "Epoch 40, Loss: 1.7409, AUC: 0.4380\n",
      "Epoch 50, Loss: 1.5398, AUC: 0.4333\n",
      "Epoch 60, Loss: 1.4832, AUC: 0.5667\n",
      "Epoch 70, Loss: 1.4806, AUC: 0.5023\n",
      "Epoch 80, Loss: 1.4767, AUC: 0.7833\n",
      "Epoch 90, Loss: 1.4787, AUC: 0.7190\n",
      "Epoch 100, Loss: 1.4758, AUC: 0.4349\n",
      "Epoch 110, Loss: 1.4768, AUC: 0.2213\n",
      "Epoch 120, Loss: 1.4781, AUC: 0.4977\n",
      "Epoch 130, Loss: 1.4778, AUC: 0.4977\n",
      "Epoch 140, Loss: 1.4805, AUC: 0.4349\n"
     ]
    }
   ],
   "source": [
    "model, data = main_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c59d043-2093-45c9-9989-5ea55a029d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def generate_weight_matrix(model, data, num_candidates, num_jobs, top_k=None):\n",
    "    \"\"\"\n",
    "    Генерирует матрицу весов между пользователями и вакансиями.\n",
    "    \n",
    "    Параметры:\n",
    "    - model (torch.nn.Module): Обученная модель GraphSAGE.\n",
    "    - data (torch_geometric.data.Data): Объект данных графа.\n",
    "    - num_candidates (int): Количество пользователей.\n",
    "    - num_jobs (int): Количество вакансий.\n",
    "    - top_k (int или None): Если задано, возвращает только топ-k вакансий для каждого пользователя.\n",
    "    \n",
    "    Возвращает:\n",
    "    - weight_matrix (np.ndarray): Матрица весов размерности (num_candidates, num_jobs).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    \n",
    "    # Извлечение эмбеддингов пользователей и вакансий\n",
    "    user_embeddings = out[:num_candidates].cpu().numpy()\n",
    "    job_embeddings = out[num_candidates + 5:num_candidates + 5 + num_jobs].cpu().numpy()  # 5 OCEAN типов\n",
    "    \n",
    "    # Нормализация эмбеддингов для вычисления косинусного сходства\n",
    "    user_norm = np.linalg.norm(user_embeddings, axis=1, keepdims=True)\n",
    "    job_norm = np.linalg.norm(job_embeddings, axis=1, keepdims=True)\n",
    "    user_embeddings_norm = user_embeddings / (user_norm + 1e-10)\n",
    "    job_embeddings_norm = job_embeddings / (job_norm + 1e-10)\n",
    "    \n",
    "    # Вычисление матрицы косинусного сходства\n",
    "    weight_matrix = np.dot(user_embeddings_norm, job_embeddings_norm.T)\n",
    "    \n",
    "    if top_k is not None:\n",
    "        # Ограничение до топ_k вакансий для каждого пользователя\n",
    "        indices = np.argsort(weight_matrix, axis=1)[:, -top_k:]\n",
    "        sorted_indices = np.argsort(indices, axis=1)\n",
    "        top_weights = np.take_along_axis(weight_matrix, indices, axis=1)\n",
    "        weight_matrix_top_k = np.zeros_like(weight_matrix)\n",
    "        for i in range(num_candidates):\n",
    "            weight_matrix_top_k[i, indices[i]] = weight_matrix[i, indices[i]]\n",
    "        return weight_matrix_top_k\n",
    "    else:\n",
    "        return weight_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb85b52-dc07-48df-af44-d9e272925bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d32f6365-cb9a-486f-b270-31735f35b2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка эмбеддингов пользователей...\n",
      "Эмбеддинги пользователей загружены: torch.Size([2000, 384])\n",
      "Загрузка эмбеддингов OCEAN типов личности...\n",
      "Эмбеддинги OCEAN загружены: torch.Size([5, 384])\n",
      "Загрузка эмбеддингов вакансий...\n",
      "Эмбеддинги вакансий загружены: torch.Size([2277, 384])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def load_embeddings(embeddings_path):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "    return embeddings\n",
    "\n",
    "def load_candidates_embeddings(data_folder, max_users=1500):\n",
    "    candidates_train_path = os.path.join(data_folder, 'embeddings', 'candidates_train_text_embeddings.npy')\n",
    "    candidates_val_path = os.path.join(data_folder, 'embeddings', 'candidates_val_text_embeddings.npy')\n",
    "    \n",
    "    candidates_train = load_embeddings(candidates_train_path)\n",
    "    candidates_val = load_embeddings(candidates_val_path)\n",
    "    \n",
    "    # Объединяем обучающие и валидационные эмбеддинги\n",
    "    candidates = torch.cat([candidates_train, candidates_val], dim=0)\n",
    "    \n",
    "    # Ограничиваем количество пользователей до max_users\n",
    "    if candidates.shape[0] > max_users:\n",
    "        candidates = candidates[:max_users]\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def load_jobs_embeddings(data_folder):\n",
    "    jobs_path = os.path.join(data_folder, 'embeddings', 'vacancies_embeddings.npy')\n",
    "    jobs_embeddings = load_embeddings(jobs_path)\n",
    "    return jobs_embeddings\n",
    "\n",
    "def load_ocean_embeddings(data_folder):\n",
    "    ocean_path = os.path.join(data_folder, 'embeddings', 'OCEAN_embeddings.npy')\n",
    "    ocean_embeddings = load_embeddings(ocean_path)\n",
    "    return ocean_embeddings\n",
    "    \n",
    "candidates_video_emb = []\n",
    "for i in os.listdir('data/validation/video_embeddings/'):\n",
    "    emb = np.load(f'data/validation/video_embeddings/{i}')\n",
    "    candidates_video_emb.append(emb[0])\n",
    "candidates_video_emb = np.array(candidates_video_emb)\n",
    "\n",
    "candidates_text_emb = []\n",
    "for i in os.listdir('data/validation/text_embeddings/'):\n",
    "    emb = np.load(f'data/validation/text_embeddings/{i}')\n",
    "    candidates_text_emb.append(emb)\n",
    "candidates_text_emb = np.array(candidates_text_emb)\n",
    "types_emb = np.load('data/embeddings/OCEAN_embeddings.npy')\n",
    "candidates_video_emb_norm = candidates_video_emb / np.linalg.norm(candidates_video_emb, axis=1, keepdims=True)\n",
    "candidates_text_emb_norm = candidates_text_emb / np.linalg.norm(candidates_text_emb, axis=1, keepdims=True)\n",
    "types_emb_norm = types_emb / np.linalg.norm(types_emb, axis=1, keepdims=True)\n",
    "\n",
    "candidates_emb = (candidates_video_emb_norm + candidates_text_emb_norm) / 2\n",
    "\n",
    "\n",
    "# Путь к папке с данными\n",
    "data_folder = 'data'\n",
    "\n",
    "# Загрузка эмбеддингов\n",
    "print(\"Загрузка эмбеддингов пользователей...\")\n",
    "user_embeddings = torch.tensor(candidates_emb, dtype=torch.float32)\n",
    "#candidates = load_candidates_embeddings(data_folder)\n",
    "print(f\"Эмбеддинги пользователей загружены: {candidates.shape}\")\n",
    "\n",
    "print(\"Загрузка эмбеддингов OCEAN типов личности...\")\n",
    "ocean_embeddings = load_ocean_embeddings(data_folder)\n",
    "print(f\"Эмбеддинги OCEAN загружены: {ocean.shape}\")\n",
    "\n",
    "print(\"Загрузка эмбеддингов вакансий...\")\n",
    "vacancy_embeddings = load_jobs_embeddings(data_folder)\n",
    "print(f\"Эмбеддинги вакансий загружены: {jobs.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "07706290-4c3f-4a08-8a2f-6a24b6e1f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "num_users = user_embeddings.shape[0]\n",
    "num_ocean = ocean_embeddings.shape[0]\n",
    "num_vacancies = vacancy_embeddings.shape[0]\n",
    "\n",
    "# Создаем список всех вершин\n",
    "num_nodes = num_users + num_ocean + num_vacancies\n",
    "\n",
    "# Создаем ребра\n",
    "edge_index = []\n",
    "edge_weight = []\n",
    "\n",
    "# Пользователь ↔ OCEAN\n",
    "user_ocean_sim = cosine_similarity(user_embeddings, ocean_embeddings)\n",
    "for user in range(num_users):\n",
    "    for ocean in range(num_ocean):\n",
    "        edge_index.append([user, num_users + ocean])\n",
    "        edge_weight.append(user_ocean_sim[user, ocean])\n",
    "\n",
    "# Вакансия ↔ OCEAN\n",
    "vacancy_ocean_sim = cosine_similarity(vacancy_embeddings, ocean_embeddings)  # (2277, 5)\n",
    "for vacancy in range(num_vacancies):\n",
    "    for ocean in range(num_ocean):\n",
    "        edge_index.append([num_users + ocean, num_users + num_ocean + vacancy])\n",
    "        edge_weight.append(1 - vacancy_ocean_sim[vacancy, ocean])  # Косинусное расстояние\n",
    "\n",
    "# Пользователь ↔ Вакансия (без весов или с начальными весами)\n",
    "# Для примера, создадим полносвязные связи (можно использовать более эффективный подход)\n",
    "# Но из-за большого количества связей это может быть неэффективно\n",
    "# Альтернативно, можно использовать выборку или другие методы для создания связей\n",
    "\n",
    "# Здесь для примера ограничимся связями только через OCEAN\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "\n",
    "# Создание графа\n",
    "data = Data(x=torch.tensor(np.vstack([user_embeddings, ocean_embeddings, vacancy_embeddings]), dtype=torch.float),\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5993b8e3-1d84-4641-8430-f9501a0fbc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GraphSAGEModel(in_channels=384, hidden_channels=128, out_channels=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cab8a094-4cca-424f-a0af-8b90069c5781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb4fe5c-bbd5-4f4c-9679-bae9d2d6486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.0020748700480908155\n",
      "Epoch 10, Loss: 0.005078461021184921\n",
      "Epoch 20, Loss: 0.005078461021184921\n"
     ]
    }
   ],
   "source": [
    "# Получение эмбеддингов после прохождения через GraphSAGE\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "    \n",
    "    # Извлекаем эмбеддинги пользователей и вакансий\n",
    "    user_out = out[:num_users]\n",
    "    vacancy_out = out[num_users + num_ocean:]\n",
    "    \n",
    "    # Вычисляем косинусное сходство между пользователями и вакансиями\n",
    "    similarity = torch.mm(user_out, vacancy_out.t())  # (1000, 2277)\n",
    "    similarity = similarity / (user_out.norm(dim=1).unsqueeze(1) * vacancy_out.norm(dim=1).unsqueeze(0))\n",
    "    similarity = similarity.clamp(0, 1)\n",
    "    \n",
    "    # Поскольку у нас нет явных меток, можно использовать косинусное сходство эмбеддингов как целевую\n",
    "    target = torch.tensor(cosine_similarity(user_embeddings, vacancy_embeddings), dtype=torch.float)\n",
    "    \n",
    "    loss = criterion(similarity, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b091470-ff69-4eea-9492-cef28f0a1b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
