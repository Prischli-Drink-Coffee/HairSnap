{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d50075db-fa5d-4293-ac8a-7fd1c84e0881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 384)\n",
      "(5, 384)\n",
      "(2277, 384)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def build_matrix(\n",
    "    candidates_path='data/embeddings/candidates_train_text_embeddings.npy',\n",
    "    ocean_path='data/embeddings/OCEAN_embeddings.npy',\n",
    "    vacancies_path='data/embeddings/vacancies_embeddings.npy'\n",
    "):\n",
    "  \n",
    "    candidates = np.load(candidates_path)\n",
    "    print(candidates.shape)\n",
    "    ocean = np.load(ocean_path)\n",
    "    print(ocean.shape)\n",
    "    vacancies = np.load(vacancies_path)\n",
    "    print(vacancies.shape)\n",
    "\n",
    "    C = cosine_distances(candidates, ocean)\n",
    "    P = cosine_distances(vacancies, ocean)\n",
    "\n",
    "    final_matrix = C.dot(P.T) + (1 - C).dot(1 - P.T)\n",
    "    \n",
    "    return final_matrix, C, P\n",
    "\n",
    "def filter_matrix(final_matrix, threshold=0.7):\n",
    "\n",
    "    normalized_matrix = final_matrix.copy()\n",
    "    \n",
    "    row_min = normalized_matrix.min(axis=1).reshape(-1, 1)\n",
    "    row_max = normalized_matrix.max(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    denominator = row_max - row_min\n",
    "    denominator[denominator == 0] = 1\n",
    "    \n",
    "    normalized_matrix = (normalized_matrix - row_min) / denominator\n",
    "    \n",
    "    normalized_matrix[normalized_matrix < threshold] = 0\n",
    "    \n",
    "    return normalized_matrix\n",
    "\n",
    "a, C, P = build_matrix()\n",
    "b = filter_matrix(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "551dd333-9748-44dc-a505-588c423853bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется устройство: Tesla V100-SXM3-32GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Используется устройство: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"GPU не доступен. Используется CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e49ad2b9-d974-4aee-beeb-869453d40ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embeddings_path):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "    return embeddings\n",
    "\n",
    "def load_candidates_embeddings(data_folder, max_users=2277):\n",
    "    candidates_train_path = os.path.join(data_folder, 'embeddings', 'candidates_train_text_embeddings.npy')\n",
    "    candidates_val_path = os.path.join(data_folder, 'embeddings', 'candidates_val_text_embeddings.npy')\n",
    "    \n",
    "    candidates_train = load_embeddings(candidates_train_path)\n",
    "    candidates_val = load_embeddings(candidates_val_path)\n",
    "    \n",
    "    # Объединяем обучающие и валидационные эмбеддинги\n",
    "    candidates = torch.cat([candidates_train, candidates_val], dim=0)\n",
    "    \n",
    "    # Ограничиваем количество пользователей до max_users\n",
    "    if candidates.shape[0] > max_users:\n",
    "        candidates = candidates[:max_users]\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def load_jobs_embeddings(data_folder):\n",
    "    jobs_path = os.path.join(data_folder, 'embeddings', 'vacancies_embeddings.npy')\n",
    "    jobs_embeddings = load_embeddings(jobs_path)\n",
    "    return jobs_embeddings\n",
    "\n",
    "def load_ocean_embeddings(data_folder):\n",
    "    ocean_path = os.path.join(data_folder, 'embeddings', 'OCEAN_embeddings.npy')\n",
    "    ocean_embeddings = load_embeddings(ocean_path)\n",
    "    return ocean_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e34308b-5914-4d53-99a3-58bc95344d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка эмбеддингов пользователей...\n",
      "Эмбеддинги пользователей загружены: torch.Size([2277, 384])\n",
      "Загрузка эмбеддингов OCEAN типов личности...\n",
      "Эмбеддинги OCEAN загружены: torch.Size([5, 384])\n",
      "Загрузка эмбеддингов вакансий...\n",
      "Эмбеддинги вакансий загружены: torch.Size([2277, 384])\n"
     ]
    }
   ],
   "source": [
    "# Путь к папке с данными\n",
    "data_folder = 'data'\n",
    "\n",
    "# Загрузка эмбеддингов\n",
    "print(\"Загрузка эмбеддингов пользователей...\")\n",
    "candidates = load_candidates_embeddings(data_folder)\n",
    "print(f\"Эмбеддинги пользователей загружены: {candidates.shape}\")\n",
    "\n",
    "print(\"Загрузка эмбеддингов OCEAN типов личности...\")\n",
    "ocean = load_ocean_embeddings(data_folder)\n",
    "print(f\"Эмбеддинги OCEAN загружены: {ocean.shape}\")\n",
    "\n",
    "print(\"Загрузка эмбеддингов вакансий...\")\n",
    "jobs = load_jobs_embeddings(data_folder)\n",
    "print(f\"Эмбеддинги вакансий загружены: {jobs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd951ec-d639-4a70-838b-0f4bdb4c9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def build_graph(candidates, ocean, jobs):\n",
    "    num_candidates = candidates.shape[0]\n",
    "    num_ocean = ocean.shape[0]\n",
    "    num_jobs = jobs.shape[0]\n",
    "    \n",
    "    # Объединяем все эмбеддинги в один\n",
    "    x = torch.cat([candidates, ocean, jobs], dim=0)\n",
    "    \n",
    "    # Создаём рёбра\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    \n",
    "    # Пользователи ↔ OCEAN\n",
    "    for user_idx in range(num_candidates):\n",
    "        for ocean_idx in range(num_ocean):\n",
    "            src = user_idx\n",
    "            dst = num_candidates + ocean_idx\n",
    "            edge_index.append([src, dst])\n",
    "            # Вычисляем косинусное расстояние\n",
    "            distance = 1 - cosine_similarity(candidates[user_idx].unsqueeze(0), ocean[ocean_idx].unsqueeze(0))[0][0]\n",
    "            edge_attr.append(distance)\n",
    "            # Добавляем обратное ребро\n",
    "            edge_index.append([dst, src])\n",
    "            edge_attr.append(distance)\n",
    "    \n",
    "    # Вакансии ↔ OCEAN\n",
    "    for job_idx in range(num_jobs):\n",
    "        for ocean_idx in range(num_ocean):\n",
    "            src = num_candidates + ocean_idx\n",
    "            dst = num_candidates + num_ocean + job_idx\n",
    "            edge_index.append([src, dst])\n",
    "            distance = 1 - cosine_similarity(jobs[job_idx].unsqueeze(0), ocean[ocean_idx].unsqueeze(0))[0][0]\n",
    "            edge_attr.append(distance)\n",
    "            # Добавляем обратное ребро\n",
    "            edge_index.append([dst, src])\n",
    "            edge_attr.append(distance)\n",
    "    \n",
    "    # Пользователи ↔ Вакансии (без весов)\n",
    "    for user_idx in range(num_candidates):\n",
    "        for job_idx in range(num_jobs):\n",
    "            src = user_idx\n",
    "            dst = num_candidates + num_ocean + job_idx\n",
    "            edge_index.append([src, dst])\n",
    "            edge_attr.append(0.0)  # Вес 0 для отсутствующих весов\n",
    "            # Добавляем обратное ребро\n",
    "            edge_index.append([dst, src])\n",
    "            edge_attr.append(0.0)\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bc53900-0851-4bf7-b5f7-5eca1b5cb3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Построение графа...\n",
      "Граф создан: Data(x=[4559, 384], edge_index=[2, 10414998], edge_attr=[10414998])\n"
     ]
    }
   ],
   "source": [
    "print(\"Построение графа...\")\n",
    "data = build_graph(candidates, ocean, jobs)\n",
    "print(f\"Граф создан: {data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f7bf71-947b-4a85-9fb2-1bbaf0bfdfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGEModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e8cdb15-f75a-4cfc-bb4e-9e26d3656b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "def get_link_labels(edge_index, num_nodes, num_neg_samples=10000):\n",
    "\n",
    "    #Генерирует положительные и отрицательные примеры для задачи предсказания рёбер.\n",
    "    #Мы будем использовать задачу Link Prediction для предсказания существования рёбер между пользователями и вакансиями.\n",
    "    \n",
    "    pos_edge_index = edge_index\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=pos_edge_index,\n",
    "        num_nodes=num_nodes,\n",
    "        num_neg_samples=num_neg_samples\n",
    "    )\n",
    "    return pos_edge_index, neg_edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c4e412-5747-48c3-a9c0-a9a343dd7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class LinkPredictionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, num_neg_samples=10000):\n",
    "        self.data = data\n",
    "        self.num_neg_samples = num_neg_samples\n",
    "        self.pos_edge_index, self.neg_edge_index = get_link_labels(\n",
    "            data.edge_index,\n",
    "            num_nodes=data.num_nodes,\n",
    "            num_neg_samples=num_neg_samples\n",
    "        )\n",
    "        # Перемещаем рёбра на GPU\n",
    "        self.pos_edge_index = self.pos_edge_index.to(device)\n",
    "        self.neg_edge_index = self.neg_edge_index.to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1  # В данном случае весь граф обучается за один раз\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pos_edge_index, self.neg_edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ede45fb0-75bc-4155-80d9-cde4fb6f3be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data, pos_edge_index, neg_edge_index):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    print(f\"out device: {out.device}\")\n",
    "    \n",
    "    pos_pred = (out[pos_edge_index[0]] * out[pos_edge_index[1]]).sum(dim=1)\n",
    "    print(f\"pos_pred device: {pos_pred.device}\")\n",
    "    \n",
    "    neg_pred = (out[neg_edge_index[0]] * out[neg_edge_index[1]]).sum(dim=1)\n",
    "    print(f\"neg_pred device: {neg_pred.device}\")\n",
    "    \n",
    "    # Логистическая регрессия\n",
    "    pos_loss = F.binary_cross_entropy_with_logits(pos_pred, torch.ones(pos_pred.size(0), device=device))\n",
    "    neg_loss = F.binary_cross_entropy_with_logits(neg_pred, torch.zeros(neg_pred.size(0), device=device))\n",
    "    print(f\"pos_loss device: {pos_loss.device}\")\n",
    "    print(f\"neg_loss device: {neg_loss.device}\")\n",
    "    \n",
    "    loss = pos_loss + neg_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def test(model, data, pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index, data.edge_attr)\n",
    "        pos_pred = (out[pos_edge_index[0]] * out[pos_edge_index[1]]).sum(dim=1)\n",
    "        neg_pred = (out[neg_edge_index[0]] * out[neg_edge_index[1]]).sum(dim=1)\n",
    "        \n",
    "        print(f\"pos_pred device: {pos_pred.device}\")\n",
    "        print(f\"neg_pred device: {neg_pred.device}\")\n",
    "        \n",
    "        preds = torch.cat([pos_pred, neg_pred], dim=0)\n",
    "        labels = torch.cat([torch.ones(pos_pred.size(0), device=device), torch.zeros(neg_pred.size(0), device=device)], dim=0)\n",
    "        \n",
    "        print(f\"preds device: {preds.device}\")\n",
    "        print(f\"labels device: {labels.device}\")\n",
    "        \n",
    "        preds = torch.sigmoid(preds)\n",
    "        preds = preds.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        \n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        auc = roc_auc_score(labels, preds)\n",
    "    \n",
    "    return auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f70e3f96-978e-44b2-91b7-89e8ecf3dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training_loop():\n",
    "    # Создание графа\n",
    "    print(\"Построение графа...\")\n",
    "    data = build_graph(candidates, ocean, jobs)\n",
    "    print(f\"Граф создан: {data}\")\n",
    "    \n",
    "    num_nodes = data.num_nodes\n",
    "    \n",
    "    # Создание датасета и загрузчика данных\n",
    "    dataset = LinkPredictionDataset(data, num_neg_samples=10000)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    # Определение модели\n",
    "    in_channels = data.x.shape[1]\n",
    "    hidden_channels = 64  # Уменьшение числа скрытых каналов\n",
    "    out_channels = 64\n",
    "    num_layers = 2\n",
    "    model = GraphSAGEModel(in_channels, hidden_channels, out_channels, num_layers).to(device)\n",
    "    \n",
    "    # Перевод данных в float16 и перемещение на GPU\n",
    "    data.x = data.x.to(device)\n",
    "    data.edge_attr = data.edge_attr.to(device)\n",
    "    data.edge_index = data.edge_index.to(device)  # Перемещение edge_index на GPU\n",
    "\n",
    "    print(f\"data.x device: {data.x.device}\")\n",
    "    print(f\"data.edge_attr device: {data.edge_attr.device}\")\n",
    "    print(f\"data.edge_index device: {data.edge_index.device}\")\n",
    "    # Оптимизатор\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Очистка памяти\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Обучение\n",
    "    epochs = 100\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for pos_edge, neg_edge in dataloader:\n",
    "            pos_edge = pos_edge[0].to(device)\n",
    "            neg_edge = neg_edge[0].to(device)\n",
    "            loss = train(model, optimizer, data, pos_edge, neg_edge)\n",
    "        if epoch % 10 == 0:\n",
    "            auc = test(model, data, pos_edge, neg_edge)\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}, AUC: {auc:.4f}')\n",
    "    \n",
    "    # Сохранение модели\n",
    "    model_save_path = 'graphsage_model.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Модель сохранена по пути: {model_save_path}\")\n",
    "    \n",
    "    return model, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f8edfe2-ca73-4dfc-bef3-393f9583dc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.x device: cpu\n",
      "data.edge_attr device: cpu\n",
      "data.edge_index device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"data.x device: {data.x.device}\")\n",
    "print(f\"data.edge_attr device: {data.edge_attr.device}\")\n",
    "print(f\"data.edge_index device: {data.edge_index.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8127a1d-a184-4d75-8175-cc60fb4783da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Построение графа...\n",
      "Граф создан: Data(x=[4559, 384], edge_index=[2, 10414998], edge_attr=[10414998])\n",
      "data.x device: cuda:0\n",
      "data.edge_attr device: cuda:0\n",
      "data.edge_index device: cuda:0\n",
      "out device: cuda:0\n",
      "pos_pred device: cuda:0\n",
      "neg_pred device: cuda:0\n",
      "pos_loss device: cuda:0\n",
      "neg_loss device: cuda:0\n",
      "out device: cuda:0\n",
      "pos_pred device: cuda:0\n",
      "neg_pred device: cuda:0\n",
      "pos_loss device: cuda:0\n",
      "neg_loss device: cuda:0\n",
      "out device: cuda:0\n",
      "pos_pred device: cuda:0\n",
      "neg_pred device: cuda:0\n",
      "pos_loss device: cuda:0\n",
      "neg_loss device: cuda:0\n",
      "out device: cuda:0\n",
      "pos_pred device: cuda:0\n",
      "neg_pred device: cuda:0\n",
      "pos_loss device: cuda:0\n",
      "neg_loss device: cuda:0\n",
      "out device: cuda:0\n",
      "pos_pred device: cuda:0\n",
      "neg_pred device: cuda:0\n",
      "pos_loss device: cuda:0\n",
      "neg_loss device: cuda:0\n",
      "out device: cuda:0\n",
      "pos_pred device: cuda:0\n",
      "neg_pred device: cuda:0\n",
      "pos_loss device: cuda:0\n",
      "neg_loss device: cuda:0\n",
      "out device: cuda:0\n",
      "pos_pred device: cuda:0\n",
      "neg_pred device: cuda:0\n",
      "pos_loss device: cuda:0\n",
      "neg_loss device: cuda:0\n",
      "out device: cuda:0\n",
      "pos_pred device: cuda:0\n",
      "neg_pred device: cuda:0\n",
      "pos_loss device: cuda:0\n",
      "neg_loss device: cuda:0\n",
      "out device: cuda:0\n",
      "pos_pred device: cuda:0\n",
      "neg_pred device: cuda:0\n",
      "pos_loss device: cuda:0\n",
      "neg_loss device: cuda:0\n",
      "out device: cuda:0\n",
      "pos_pred device: cuda:0\n",
      "neg_pred device: cuda:0\n",
      "pos_loss device: cuda:0\n",
      "neg_loss device: cuda:0\n",
      "pos_pred device: cuda:0\n",
      "neg_pred device: cuda:0\n",
      "preds device: cuda:0\n",
      "labels device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model, data = main_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c59d043-2093-45c9-9989-5ea55a029d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def generate_weight_matrix(model, data, num_candidates, num_jobs, top_k=None):\n",
    "    \"\"\"\n",
    "    Генерирует матрицу весов между пользователями и вакансиями.\n",
    "    \n",
    "    Параметры:\n",
    "    - model (torch.nn.Module): Обученная модель GraphSAGE.\n",
    "    - data (torch_geometric.data.Data): Объект данных графа.\n",
    "    - num_candidates (int): Количество пользователей.\n",
    "    - num_jobs (int): Количество вакансий.\n",
    "    - top_k (int или None): Если задано, возвращает только топ-k вакансий для каждого пользователя.\n",
    "    \n",
    "    Возвращает:\n",
    "    - weight_matrix (np.ndarray): Матрица весов размерности (num_candidates, num_jobs).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    \n",
    "    # Извлечение эмбеддингов пользователей и вакансий\n",
    "    user_embeddings = out[:num_candidates].cpu().numpy()\n",
    "    job_embeddings = out[num_candidates + 5:num_candidates + 5 + num_jobs].cpu().numpy()  # 5 OCEAN типов\n",
    "    \n",
    "    # Нормализация эмбеддингов для вычисления косинусного сходства\n",
    "    user_norm = np.linalg.norm(user_embeddings, axis=1, keepdims=True)\n",
    "    job_norm = np.linalg.norm(job_embeddings, axis=1, keepdims=True)\n",
    "    user_embeddings_norm = user_embeddings / (user_norm + 1e-10)\n",
    "    job_embeddings_norm = job_embeddings / (job_norm + 1e-10)\n",
    "    \n",
    "    # Вычисление матрицы косинусного сходства\n",
    "    weight_matrix = np.dot(user_embeddings_norm, job_embeddings_norm.T)\n",
    "    \n",
    "    if top_k is not None:\n",
    "        # Ограничение до топ_k вакансий для каждого пользователя\n",
    "        indices = np.argsort(weight_matrix, axis=1)[:, -top_k:]\n",
    "        sorted_indices = np.argsort(indices, axis=1)\n",
    "        top_weights = np.take_along_axis(weight_matrix, indices, axis=1)\n",
    "        weight_matrix_top_k = np.zeros_like(weight_matrix)\n",
    "        for i in range(num_candidates):\n",
    "            weight_matrix_top_k[i, indices[i]] = weight_matrix[i, indices[i]]\n",
    "        return weight_matrix_top_k\n",
    "    else:\n",
    "        return weight_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e94158-310f-4225-b5a4-cbd5f1a53b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Загрузка эмбеддингов\n",
    "user_embeddings = np.load('data/embeddings/candidates_train_text_embeddings.npy')  # (6000, 384)\n",
    "ocean_embeddings = np.load('data/embeddings/OCEAN_embeddings.npy')  # (5, 384)\n",
    "vacancy_embeddings = np.load('data/embeddings/vacancies_embeddings.npy')  # (2277, 384)\n",
    "test_user_embeddings = np.load('data/embeddings/candidates_val_text_embeddings.npy')  # форма зависит от данных\n",
    "selected_user_embeddings = user_embeddings[:2277]  # (2277, 384)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a7a31e-8481-4cd7-8c79-b394a3997fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 2277\n",
    "num_ocean = 5\n",
    "num_vacancies = 2277\n",
    "total_nodes = num_users + num_ocean + num_vacancies\n",
    "\n",
    "# Индексы узлов\n",
    "user_indices = torch.arange(0, num_users, dtype=torch.long)\n",
    "ocean_indices = torch.arange(num_users, num_users + num_ocean, dtype=torch.long)\n",
    "vacancy_indices = torch.arange(num_users + num_ocean, total_nodes, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377597ae-c887-4319-bee6-3c7f82269c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Шаг 1: Загрузка и нормализация эмбеддингов\n",
    "def load_and_normalize_embeddings():\n",
    "    user_embeddings = np.load('data/embeddings/candidates_train_text_embeddings.npy')[:2277]  # (2277, 384)\n",
    "    ocean_embeddings = np.load('data/embeddings/OCEAN_embeddings.npy')  # (5, 384)\n",
    "    vacancy_embeddings = np.load('data/embeddings/vacancies_embeddings.npy')  # (2277, 384)\n",
    "    test_user_embeddings = np.load('data/embeddings/candidates_val_text_embeddings.npy')  # (N_test, 384)\n",
    "    print(test_user_embeddings.shape)\n",
    "    # Нормализация эмбеддингов по строкам\n",
    "    user_embeddings = normalize(user_embeddings, axis=1)\n",
    "    ocean_embeddings = normalize(ocean_embeddings, axis=1)\n",
    "    vacancy_embeddings = normalize(vacancy_embeddings, axis=1)\n",
    "    test_user_embeddings = normalize(test_user_embeddings, axis=1)\n",
    "    \n",
    "    return user_embeddings, ocean_embeddings, vacancy_embeddings, test_user_embeddings\n",
    "\n",
    "# Шаг 2: Присвоение индексов узлам\n",
    "def assign_node_indices(num_users, num_ocean, num_vacancies):\n",
    "    total_nodes = num_users + num_ocean + num_vacancies\n",
    "    return total_nodes\n",
    "\n",
    "# Шаг 3: Построение рёбер\n",
    "def build_edges(user_embeddings, ocean_embeddings, vacancy_embeddings, num_users, num_ocean, num_vacancies):\n",
    "    # 3.1. Пользователи ↔ OCEAN\n",
    "    user_ocean_sim = cosine_similarity(user_embeddings, ocean_embeddings)  # (2277, 5)\n",
    "    source = []\n",
    "    target = []\n",
    "    edge_weights = []\n",
    "    \n",
    "    for u in range(num_users):\n",
    "        for o in range(num_ocean):\n",
    "            sim = user_ocean_sim[u, o]\n",
    "            source.append(u)\n",
    "            target.append(num_users + o)\n",
    "            edge_weights.append(sim)\n",
    "            # Обратные рёбра\n",
    "            source.append(num_users + o)\n",
    "            target.append(u)\n",
    "            edge_weights.append(sim)\n",
    "    \n",
    "    edge_index_user_ocean = torch.tensor([source, target], dtype=torch.long)\n",
    "    edge_weight_user_ocean = torch.tensor(edge_weights, dtype=torch.float)\n",
    "    \n",
    "    # 3.2. Вакансии ↔ OCEAN\n",
    "    vacancy_ocean_sim = cosine_similarity(vacancy_embeddings, ocean_embeddings)  # (2277, 5)\n",
    "    vacancy_ocean_dist = 1 - vacancy_ocean_sim  # (2277, 5)\n",
    "    source = []\n",
    "    target = []\n",
    "    edge_weights_vacancy_ocean = []\n",
    "    \n",
    "    for v in range(num_vacancies):\n",
    "        for o in range(num_ocean):\n",
    "            dist = vacancy_ocean_dist[v, o]\n",
    "            source.append(num_users + num_ocean + v)\n",
    "            target.append(num_users + o)\n",
    "            edge_weights_vacancy_ocean.append(dist)\n",
    "            # Обратные рёбра\n",
    "            source.append(num_users + o)\n",
    "            target.append(num_users + num_ocean + v)\n",
    "            edge_weights_vacancy_ocean.append(dist)\n",
    "    \n",
    "    edge_index_vacancy_ocean = torch.tensor([source, target], dtype=torch.long)\n",
    "    edge_weight_vacancy_ocean = torch.tensor(edge_weights_vacancy_ocean, dtype=torch.float)\n",
    "    \n",
    "    # 3.3. Пользователи ↔ Вакансии (все связи)\n",
    "    source = []\n",
    "    target = []\n",
    "    edge_weights_user_vacancy = []\n",
    "    \n",
    "    for u in range(num_users):\n",
    "        for v in range(num_vacancies):\n",
    "            source.append(u)\n",
    "            target.append(num_users + num_ocean + v)\n",
    "            edge_weights_user_vacancy.append(1.0)  # Вес по умолчанию\n",
    "            # Обратные рёбра\n",
    "            source.append(num_users + num_ocean + v)\n",
    "            target.append(u)\n",
    "            edge_weights_user_vacancy.append(1.0)\n",
    "    \n",
    "    edge_index_user_vacancy = torch.tensor([source, target], dtype=torch.long)\n",
    "    edge_weight_user_vacancy = torch.tensor(edge_weights_user_vacancy, dtype=torch.float)\n",
    "    \n",
    "    # Очистка памяти (опционально)\n",
    "    del source, target, edge_weights, edge_weights_vacancy_ocean, edge_weights_user_vacancy\n",
    "    \n",
    "    # 3.4. Объединение всех рёбер\n",
    "    edge_index = torch.cat([edge_index_user_ocean, edge_index_vacancy_ocean, edge_index_user_vacancy], dim=1)\n",
    "    edge_weight = torch.cat([edge_weight_user_ocean, edge_weight_vacancy_ocean, edge_weight_user_vacancy], dim=0)\n",
    "    \n",
    "    return edge_index, edge_weight\n",
    "\n",
    "# Шаг 4: Создание объекта Data для PyG\n",
    "def create_pyg_data(edge_index, edge_weight, user_embeddings, ocean_embeddings, vacancy_embeddings):\n",
    "    all_embeddings = np.concatenate([user_embeddings, ocean_embeddings, vacancy_embeddings], axis=0)  # (4559, 384)\n",
    "    data = Data(edge_index=edge_index, edge_attr=edge_weight, x=torch.tensor(all_embeddings, dtype=torch.float))\n",
    "    return data\n",
    "\n",
    "# Шаг 5: Определение кастомного слоя GraphSAGE с учётом весов рёбер\n",
    "class SAGEConvWithEdgeWeight(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, aggr='mean'):\n",
    "        super(SAGEConvWithEdgeWeight, self).__init__(aggr=aggr)  # \"Add\", \"Mean\", \"Max\"\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.edge_lin = torch.nn.Linear(1, out_channels, bias=False)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        # x: [N, in_channels]\n",
    "        # edge_index: [2, E]\n",
    "        # edge_weight: [E]\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), device=x.device)\n",
    "        \n",
    "        # Reshape edge_weight to [E, 1]\n",
    "        edge_weight = edge_weight.view(-1, 1)\n",
    "        \n",
    "        # Линейное преобразование весов рёбер\n",
    "        edge_weight = self.edge_lin(edge_weight)  # [E, out_channels]\n",
    "        \n",
    "        # Преобразование признаков узлов\n",
    "        x = self.lin(x)  # [N, out_channels]\n",
    "        \n",
    "        return self.propagate(edge_index, x=x, edge_weight=edge_weight)\n",
    "    \n",
    "    def message(self, x_j, edge_weight):\n",
    "        # x_j: [E, out_channels]\n",
    "        # edge_weight: [E, out_channels]\n",
    "        return x_j * edge_weight  # [E, out_channels]\n",
    "    \n",
    "    def update(self, aggr_out):\n",
    "        return self.activation(aggr_out)\n",
    "\n",
    "# Шаг 6: Определение модели GraphSAGE\n",
    "class GraphSAGEModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.conv1 = SAGEConvWithEdgeWeight(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConvWithEdgeWeight(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConvWithEdgeWeight(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv3(x, edge_index, edge_weight=edge_weight)\n",
    "        return x\n",
    "\n",
    "# Шаг 7: Создание меток и подготовка датасета\n",
    "def prepare_labels_and_dataset(num_users, num_vacancies):\n",
    "    # Создание меток (используйте реальные данные вместо случайных)\n",
    "    labels = torch.randint(0, 2, (num_users, num_vacancies)).float()\n",
    "    \n",
    "    # Создание списков пар пользователь-вакансия\n",
    "    user_ids = []\n",
    "    vacancy_ids = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for u in range(num_users):\n",
    "        for v in range(num_vacancies):\n",
    "            user_ids.append(u)\n",
    "            vacancy_ids.append(v)\n",
    "            train_labels.append(labels[u, v])\n",
    "    \n",
    "    user_ids = torch.tensor(user_ids, dtype=torch.long)\n",
    "    vacancy_ids = torch.tensor(vacancy_ids, dtype=torch.long)\n",
    "    train_labels = torch.tensor(train_labels, dtype=torch.float)\n",
    "    \n",
    "    # Разделение на тренировочную и валидационную выборки\n",
    "    train_u, val_u, train_v, val_v, y_train, y_val = train_test_split(\n",
    "        user_ids, vacancy_ids, train_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    return train_u, val_u, train_v, val_v, y_train, y_val, labels\n",
    "\n",
    "# Шаг 8: Инициализация модели, оптимизатора и функции потерь\n",
    "def initialize_model_and_optimizer(in_channels, hidden_channels, out_channels, dropout, device):\n",
    "    model = GraphSAGEModel(in_channels, hidden_channels, out_channels, dropout).to(device)\n",
    "    return model\n",
    "\n",
    "# Шаг 9: Обучающий цикл\n",
    "def train_model(model, data, train_u, train_v, y_train, optimizer, criterion, device, num_epochs=100, batch_size=64):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        permutation = torch.randperm(len(train_u))\n",
    "        \n",
    "        for i in range(0, len(train_u), batch_size):\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_u = train_u[indices].to(device)\n",
    "            batch_v = train_v[indices].to(device)\n",
    "            batch_y = y_train[indices].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "\n",
    "            # Получение эмбеддингов пользователей и вакансий\n",
    "            user_emb = out[batch_u]  # [batch_size, out_channels]\n",
    "            vacancy_emb = out[num_users + num_ocean + batch_v]  # [batch_size, out_channels]\n",
    "\n",
    "            # Предсказание соответствия через скалярное произведение\n",
    "            scores = (user_emb * vacancy_emb).sum(dim=1)  # [batch_size]\n",
    "\n",
    "            loss = criterion(scores, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / (len(train_u) / batch_size)\n",
    "        print(f'Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Валидация каждые 10 эпох или в первой эпохе\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "                user_emb_val = out[val_u].to(device)\n",
    "                vacancy_emb_val = out[num_users + num_ocean + val_v].to(device)\n",
    "                scores_val = (user_emb_val * vacancy_emb_val).sum(dim=1)\n",
    "                preds_val = torch.sigmoid(scores_val).cpu()\n",
    "                auc = roc_auc_score(y_val.cpu(), preds_val.cpu())\n",
    "                print(f'Validation AUC: {auc:.4f}')\n",
    "\n",
    "# Шаг 10: Предсказание и построение матрицы соответствий\n",
    "def predict_suitability_matrix(model, data, num_users, num_ocean, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "        user_emb = out[:num_users]  # [num_users, out_channels]\n",
    "        vacancy_emb = out[num_users + num_ocean:]  # [num_vacancies, out_channels]\n",
    "\n",
    "        # Нормализация эмбеддингов для косинусного сходства\n",
    "        user_emb = F.normalize(user_emb, p=2, dim=1)\n",
    "        vacancy_emb = F.normalize(vacancy_emb, p=2, dim=1)\n",
    "\n",
    "        # Вычисление матрицы соответствий\n",
    "        suitability_matrix = torch.matmul(user_emb, vacancy_emb.T)  # [num_users, num_vacancies]\n",
    "        suitability_matrix = torch.sigmoid(suitability_matrix)\n",
    "        suitability_matrix = suitability_matrix.cpu().numpy()\n",
    "    \n",
    "    return suitability_matrix\n",
    "\n",
    "# Шаг 11: Оценка модели с помощью Precision@K\n",
    "def precision_at_k(suitability_matrix, true_labels, k=10):\n",
    "    num_users = suitability_matrix.shape[0]\n",
    "    precision = 0.0\n",
    "    for u in range(num_users):\n",
    "        top_k = np.argsort(suitability_matrix[u])[::-1][:k]\n",
    "        true_positives = np.sum(true_labels[u, top_k])\n",
    "        precision += true_positives / k\n",
    "    return precision / num_users\n",
    "\n",
    "# Шаг 12: Предсказание на тестовом наборе данных\n",
    "def predict_test_set(model, data, test_user_embeddings, ocean_embeddings, num_users, num_ocean, num_vacancies, device):\n",
    "    # Добавление тестовых пользователей в граф\n",
    "    test_num_users = test_user_embeddings.size(0)\n",
    "    test_total_nodes = total_nodes + test_num_users\n",
    "    \n",
    "    # Обновление эмбеддингов узлов\n",
    "    all_embeddings = torch.cat([data.x, test_user_embeddings.cpu()], dim=0).to(device)\n",
    "    \n",
    "    # Создание новых рёбер для тестовых пользователей ↔ OCEAN\n",
    "    source = []\n",
    "    target = []\n",
    "    edge_weights_test_user_ocean = []\n",
    "    \n",
    "    for u in range(test_num_users):\n",
    "        global_u = total_nodes + u\n",
    "        for o in range(num_ocean):\n",
    "            # Вычисление косинусного сходства\n",
    "            cosine_sim = cosine_similarity(test_user_embeddings[u].cpu().numpy().reshape(1, -1), ocean_embeddings)[0, o]\n",
    "            source.append(global_u)\n",
    "            target.append(num_users + o)\n",
    "            edge_weights_test_user_ocean.append(cosine_sim)\n",
    "            # Обратные рёбра\n",
    "            source.append(num_users + o)\n",
    "            target.append(global_u)\n",
    "            edge_weights_test_user_ocean.append(cosine_sim)\n",
    "    \n",
    "    edge_index_test_user_ocean = torch.tensor([source, target], dtype=torch.long)\n",
    "    edge_weight_test_user_ocean = torch.tensor(edge_weights_test_user_ocean, dtype=torch.float)\n",
    "    \n",
    "    # Создание рёбер тестовых пользователей ↔ Вакансии\n",
    "    source = []\n",
    "    target = []\n",
    "    edge_weights_test_user_vacancy = []\n",
    "    \n",
    "    for u in range(test_num_users):\n",
    "        global_u = total_nodes + u\n",
    "        for v in range(num_vacancies):\n",
    "            source.append(global_u)\n",
    "            target.append(num_users + num_ocean + v)\n",
    "            edge_weights_test_user_vacancy.append(1.0)  # Вес по умолчанию\n",
    "            # Обратные рёбра\n",
    "            source.append(num_users + num_ocean + v)\n",
    "            target.append(global_u)\n",
    "            edge_weights_test_user_vacancy.append(1.0)\n",
    "    \n",
    "    edge_index_test_user_vacancy = torch.tensor([source, target], dtype=torch.long)\n",
    "    edge_weight_test_user_vacancy = torch.tensor(edge_weights_test_user_vacancy, dtype=torch.float)\n",
    "    \n",
    "    # Очистка памяти (опционально)\n",
    "    del source, target, edge_weights_test_user_ocean, edge_weights_test_user_vacancy\n",
    "    \n",
    "    # Объединение новых рёбер\n",
    "    new_edge_index = torch.cat([edge_index_test_user_ocean, edge_index_test_user_vacancy], dim=1)\n",
    "    new_edge_weight = torch.cat([edge_weight_test_user_ocean, edge_weight_test_user_vacancy], dim=0)\n",
    "    \n",
    "    # Объединение с существующими рёбрами\n",
    "    edge_index_extended = torch.cat([data.edge_index, new_edge_index], dim=1)\n",
    "    edge_weight_extended = torch.cat([data.edge_attr, new_edge_weight], dim=0)\n",
    "    \n",
    "    # Создание обновленного графа\n",
    "    test_data = Data(edge_index=edge_index_extended, edge_attr=edge_weight_extended, x=all_embeddings)\n",
    "    test_data = test_data.to(device)\n",
    "    \n",
    "    # Предсказание\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(test_data.x, test_data.edge_index, edge_weight=test_data.edge_attr)\n",
    "    \n",
    "        # Эмбеддинги тестовых пользователей находятся в последних test_num_users узлах\n",
    "        test_user_emb = out[-test_num_users:]  # [N_test, out_channels]\n",
    "        vacancy_emb = out[num_users + num_ocean:]  # [num_vacancies, out_channels]\n",
    "    \n",
    "        # Нормализация эмбеддингов\n",
    "        test_user_emb = F.normalize(test_user_emb, p=2, dim=1)\n",
    "        vacancy_emb = F.normalize(vacancy_emb, p=2, dim=1)\n",
    "    \n",
    "        # Вычисление матрицы соответствий\n",
    "        suitability_matrix_test = torch.matmul(test_user_emb, vacancy_emb.T)  # [N_test, num_vacancies]\n",
    "        suitability_matrix_test = torch.sigmoid(suitability_matrix_test)\n",
    "        suitability_matrix_test = suitability_matrix_test.cpu().numpy()\n",
    "    \n",
    "    return suitability_matrix_test\n",
    "\n",
    "# Основная функция для выполнения всех шагов\n",
    "if __name__ == \"__main__\":\n",
    "    # Параметры\n",
    "    num_users = 2277\n",
    "    num_ocean = 5\n",
    "    num_vacancies = 2277\n",
    "    total_nodes = assign_node_indices(num_users, num_ocean, num_vacancies)\n",
    "    \n",
    "    # Шаг 1: Загрузка и нормализация эмбеддингов\n",
    "    user_embeddings, ocean_embeddings, vacancy_embeddings, test_user_embeddings = load_and_normalize_embeddings()\n",
    "    \n",
    "    # Шаг 3: Построение рёбер\n",
    "    edge_index, edge_weight = build_edges(user_embeddings, ocean_embeddings, vacancy_embeddings, num_users, num_ocean, num_vacancies)\n",
    "    \n",
    "    # Шаг 4: Создание объекта Data для PyG\n",
    "    data = create_pyg_data(edge_index, edge_weight, user_embeddings, ocean_embeddings, vacancy_embeddings)\n",
    "    \n",
    "    # Шаг 5: Определение кастомного слоя GraphSAGE с учётом весов рёбер (определено ранее)\n",
    "    \n",
    "    # Шаг 6: Определение модели GraphSAGE\n",
    "    in_channels = 384\n",
    "    hidden_channels = 128\n",
    "    out_channels = 128\n",
    "    dropout = 0.5\n",
    "    \n",
    "    # Шаг 7: Создание меток и подготовка датасета\n",
    "    train_u, val_u, train_v, val_v, y_train, y_val, labels = prepare_labels_and_dataset(num_users, num_vacancies)\n",
    "    \n",
    "    # Шаг 8: Инициализация модели, оптимизатора и функции потерь\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Используемый устройство: {device}')\n",
    "    \n",
    "    model = initialize_model_and_optimizer(in_channels, hidden_channels, out_channels, dropout, device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Перемещение данных на устройство (GPU)\n",
    "    data = data.to(device)\n",
    "    train_u = train_u.to(device)\n",
    "    train_v = train_v.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    val_u = val_u.to(device)\n",
    "    val_v = val_v.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "    \n",
    "    # Шаг 9: Обучающий цикл\n",
    "    train_model(model, data, train_u, train_v, y_train, optimizer, criterion, device, num_epochs=100, batch_size=64)\n",
    "    \n",
    "    # Шаг 10: Предсказание и построение матрицы соответствий\n",
    "    suitability_matrix = predict_suitability_matrix(model, data, num_users, num_ocean, device)\n",
    "    \n",
    "    # Шаг 11: Оценка модели с помощью Precision@K\n",
    "    precision_k = precision_at_k(suitability_matrix, labels.cpu().numpy(), k=10)\n",
    "    print(f'Precision@10: {precision_k:.4f}')\n",
    "    \n",
    "    # Шаг 12: Предсказание на тестовом наборе данных\n",
    "    suitability_matrix_test = predict_test_set(model, data, test_user_embeddings, ocean_embeddings, num_users, num_ocean, num_vacancies, device)\n",
    "    \n",
    "    # suitability_matrix_test теперь содержит значения соответствия пользователей и вакансий в диапазоне [0, 1]\n",
    "    # Вы можете сохранить эту матрицу или использовать её для дальнейших рекомендаций\n",
    "    # Например, сохранение:\n",
    "    # np.save('suitability_matrix_test.npy', suitability_matrix_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb85b52-dc07-48df-af44-d9e272925bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f6365-cb9a-486f-b270-31735f35b2b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
